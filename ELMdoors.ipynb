{
  "cells": [
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 1,
   "source": [
    "This is the code for ELM doors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the final layer\n",
    "def softmax(a):\n",
    "    c = np.max(a, axis=-1)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a, axis=-1)\n",
    "    return exp_a / sum_exp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # instantiation\n",
    "    \n",
    "    n_input_nodes = \n",
    "    n_hidden_nodes = \n",
    "    n_output_nodes = 2\n",
    "    \n",
    "    elm = ELM(\n",
    "        n_input_nodes = n_input_nodes,\n",
    "        n_hidden_nodes = n_hidden_nodes,\n",
    "        n_output_nodes = n_output_nodes,\n",
    "        \n",
    "        loss = 'mean_squared_error',\n",
    "        # default = 'mean_squared_error'\n",
    "        # more options = 'mean _absolute error', 'categorical_crossentropy', and 'binary_crossentropy'\n",
    "        activation = 'sigmoid',\n",
    "    \n",
    "    )\n",
    "    # preparation of dataset\n",
    "\n",
    "    n_classes = n_output_nodes\n",
    "\n",
    "    # load dataset\n",
    "    (x_train, t_train), (x_test, t_test) = \n",
    "    # normalise images' values within [0, 1]\n",
    "    x_train = x_train.reshape(-1, n_input_nodes) / 255.\n",
    "    x_test = x_test.reshape(-1, n_input_nodes) / 255.\n",
    "    x_train = x_train.astype(np.float32)\n",
    "    x_test = x_test.astype(np.float32)\n",
    "\n",
    "    # convert label data into one-hot-vector format data.\n",
    "    t_train = to_categorical(t_train, num_classes=n_classes)\n",
    "    t_test = to_categorical(t_test, num_classes=n_classes)\n",
    "    t_train = t_train.astype(np.float32)\n",
    "    t_test = t_test.astype(np.float32)\n",
    "\n",
    "    # divide the training dataset into two datasets:\n",
    "    # (1) for the initial training phase\n",
    "    # (2) for the sequential training phase\n",
    "    # NOTE: the number of training samples for the initial training phase\n",
    "    # must be much greater than the number of the model's hidden nodes.\n",
    "    # for example, assign int(1.5 * n_hidden_nodes) training samples\n",
    "    # for the initial training phase.\n",
    "    border = int(1.5 * n_hidden_nodes)\n",
    "    x_train_init = x_train[:border]\n",
    "    x_train_seq = x_train[border:]\n",
    "    t_train_init = t_train[:border]\n",
    "    t_train_seq = t_train[border:]\n",
    "    \n",
    "    \n",
    "    # training\n",
    "\n",
    "    # initial\n",
    "    pbar = tqdm.tqdm(total=len(x_train), desc='initial training phase')\n",
    "    elm.init_train(x_train_init, t_train_init)\n",
    "    pbar.update(n=len(x_train_init))\n",
    "\n",
    "    # sequential\n",
    "    pbar.set_description('sequential training phase')\n",
    "    batch_size = 64\n",
    "    for i in range(0, len(x_train_seq), batch_size):\n",
    "        x_batch = x_train_seq[i:i+batch_size]\n",
    "        t_batch = t_train_seq[i:i+batch_size]\n",
    "        elm.seq_train(x_batch, t_batch)\n",
    "        pbar.update(n=len(x_batch))\n",
    "    pbar.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # prediction\n",
    "\n",
    "# sample 10 validation samples from x_test\n",
    "    n = 10\n",
    "    x = x_test[:n]\n",
    "    t = t_test[:n]\n",
    "\n",
    "    # 'predict' method returns raw values of output nodes.\n",
    "    y = elm.predict(x)\n",
    "    # apply softmax function to the output values.\n",
    "    y = softmax(y)\n",
    "    print(y)\n",
    "    # check the answers.\n",
    "    for i in range(n):\n",
    "        max_ind = np.argmax(y[i])\n",
    "        print('========== sample index %d ==========' % i)\n",
    "        print('estimated answer: class %d' % max_ind)\n",
    "        print('estimated probability: %.3f' % y[i,max_ind])\n",
    "        print('true answer: class %d' % np.argmax(t[i]))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # evaluation\n",
    "\n",
    "    # loss = elm.evaluate(x_test, t_test, metrics=['loss']\n",
    "    [loss, accuracy] = elm.evaluate(x_test, t_test, metrics=['loss', 'accuracy'])\n",
    "    print('val_loss: %f, val_accuracy: %f' % (loss, accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # save model (to save weights)\n",
    "\n",
    "    print('saving model parameters...')\n",
    "    elm.save('./checkpoint/model.ckpt')\n",
    "\n",
    "    # initialize weights of os_elm\n",
    "    elm.initialize_variables()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # load model\n",
    "\n",
    "    # architecture of the model must be same as the one when the weights were saved\n",
    "\n",
    "    print('restoring model parameters...')\n",
    "    elm.restore('./checkpoint/model.ckpt')\n",
    "\n",
    "\n",
    "\n",
    "    # re-evaluation\n",
    "    \n",
    "    # loss = elm.evaluate(x_test, t_test, metrics=['loss']\n",
    "    [loss, accuracy] = elm.evaluate(x_test, t_test, metrics=['loss', 'accuracy'])\n",
    "    print('val_loss: %f, val_accuracy: %f' % (loss, accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
